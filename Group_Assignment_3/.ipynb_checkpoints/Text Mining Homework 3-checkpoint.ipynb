{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Homework 3 Text Mining\n",
    "###Samruddhi Somani, Julia Wu, Daniel Lai, Andrew Reuben, Leon Chen, Bejan Sadeghian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part A\n",
    "This portion imports the data and creates the di-graph network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "from textblob import Word\n",
    "from textblob.base import BaseTokenizer\n",
    "from textblob import TextBlob\n",
    "from textblob import Blobber\n",
    "from textblob.tokenizers import WordTokenizer\n",
    "\n",
    "from numpy import isnan\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "rawsentiment = pd.read_csv(r'Assignment 3 Sentiment Scores (2).csv')\n",
    "\n",
    "\n",
    "########\n",
    "#Part A#\n",
    "########\n",
    "\n",
    "#names = ['ES-LS','ES-RX','ES-A8','ES-A6','ES-3series','ES-5series','ES-7series','ES-XJ','ES-Sclass','LS-RX','LS-A8','LS-A6','LS-3series','LS-5series','LS-7series','LS-XJ','LS-Sclass','RX-A8','RX-A6','RX-3series','RX-5series','RX-7series','RX-XJ','RX-Sclass','A8-A6','A8-3series','A8-5series','A8-7series','A8-XJ','A8-Sclass','A6-3series','A6-5series','A6-7series','A6-XJ','A6-Sclass','3series-5series','3series-7series','3series-XJ','3series-Sclass','5series-7series','5series-XJ','5series-Sclass','7series-XJ','7series-Sclass','XJ-Sclass']\n",
    "names = list(rawsentiment.columns)\n",
    "#i=1\n",
    "#j=1\n",
    "\n",
    "##Create the sentiment difference dataframe (ES-LS, ES-RX, etc)\n",
    "length_senti = len(rawsentiment)\n",
    "width_senti = len(rawsentiment.T)\n",
    "comparisons = pd.DataFrame(index=np.array(range(length_senti)))\n",
    "#Vectors for future use in the nodes\n",
    "node_name_1 = []\n",
    "node_name_2 = []\n",
    "\n",
    "n=1 #For counter\n",
    "for i in range(width_senti-1):\n",
    "    for j in range(width_senti - n):\n",
    "        comparisons[str(names[i]) + '-' + str(names[(j+n)])] = np.array(rawsentiment[[i]]) - np.array(rawsentiment[[j+n]])\n",
    "        node_name_1.append(names[i])\n",
    "        node_name_2.append(names[(j+n)])\n",
    "    n=n+1\n",
    "\n",
    "length_comp = len(comparisons.T)\n",
    "\n",
    "averages_df = pd.DataFrame(index=comparisons.columns, columns = ['Positive','Negative'])\n",
    "\n",
    "\n",
    "\n",
    "for i in comparisons.columns:\n",
    "    pos_sum = 0\n",
    "    neg_sum = 0    \n",
    "    pos_sum = [pos_sum + x for x in comparisons[i] if x > 0]\n",
    "    neg_sum = [neg_sum + x for x in comparisons[i] if x < 0]\n",
    "    if len(pos_sum) != 0:    \n",
    "        averages_df.loc[i,'Positive'] = sum(pos_sum)/len(pos_sum)\n",
    "    if len(neg_sum) != 0:\n",
    "        averages_df.loc[i,'Negative'] = sum(neg_sum)/len(neg_sum)\n",
    "\n",
    "averages_df['Node 1'] = node_name_1\n",
    "averages_df['Node 2'] = node_name_2\n",
    "\n",
    "##Creating the network graph\n",
    "G=nx.DiGraph()\n",
    "\n",
    "#Creating the labels for the nodes and adding a node to the G object\n",
    "labels = {}\n",
    "opp_labels = {}\n",
    "for i, value in enumerate(names):\n",
    "    labels[i] = str(value)\n",
    "    opp_labels[value] = i\n",
    "    G.add_node(i) #Adding the nodes\n",
    "    \n",
    "\n",
    "#Create the list of tuples for edges\n",
    "node_list = []\n",
    "for index, pref in enumerate(averages_df.loc[:,'Positive']): #Positives\n",
    "    if ~np.isnan(pref):\n",
    "        node_list.append((pd.Series(averages_df.loc[:,'Node 2'])[index],pd.Series(averages_df.loc[:,'Node 1'])[index]))\n",
    "        G.add_edge(*(opp_labels[pd.Series(averages_df.loc[:,'Node 2'])[index]],opp_labels[pd.Series(averages_df.loc[:,'Node 1'])[index]]))\n",
    "for index, pref in enumerate(averages_df.loc[:,'Negative']): #Negatives\n",
    "    if ~np.isnan(pref):\n",
    "        node_list.append((pd.Series(averages_df.loc[:,'Node 1'])[index],pd.Series(averages_df.loc[:,'Node 2'])[index]))\n",
    "        G.add_edge(*(opp_labels[pd.Series(averages_df.loc[:,'Node 1'])[index]],opp_labels[pd.Series(averages_df.loc[:,'Node 2'])[index]]))\n",
    "\n",
    "\n",
    "pos = nx.circular_layout(G) #Positions\n",
    "\n",
    "#nx.edges([(1,2),(1,0)])#node_list)\n",
    "\n",
    "nx.draw_networkx_labels(G,pos, labels, font_size=16)\n",
    "#nx.draw_networkx_nodes(G,pos,node_color='skyblue')\n",
    "nx.draw(G,pos,node_size=1000)\n",
    "###nx.Graph(node_list)\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "plt.figure(num=None, figsize=(30,20))\n",
    "#plt.savefig('network.png', dpi=100)\n",
    "\n",
    "plt.show()\n",
    "fig1.savefig('network.png', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part B\n",
    "Part B we wrote our own code for finding eigenvalues and also wrote our own code to calculate the weighted pagerank values. This portion also gives the calculations for unweighted pagerank and correlates both with the sales of cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########\n",
    "#Part B#\n",
    "########\n",
    "\n",
    "##Coding pagerank ------- Weighted\n",
    "\n",
    "\n",
    "#Weights out\n",
    "wout_dict = {}\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if ~np.isnan(value):\n",
    "        if averages_df.ix[index,'Node 2'] in wout_dict:\n",
    "            wout_dict[averages_df.ix[index,'Node 2']] = wout_dict[averages_df.ix[index,'Node 2']] + 1\n",
    "        else:\n",
    "            wout_dict[averages_df.ix[index,'Node 2']] = 1\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if ~np.isnan(value):\n",
    "        if averages_df.ix[index,'Node 1'] in wout_dict:\n",
    "            wout_dict[averages_df.ix[index,'Node 1']] = wout_dict[averages_df.ix[index,'Node 1']] + 1\n",
    "        else:\n",
    "            wout_dict[averages_df.ix[index,'Node 1']] = 1\n",
    "\n",
    "#Weights in\n",
    "win_dict = {}\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if ~np.isnan(value):\n",
    "        if averages_df.ix[index,'Node 1'] in win_dict:\n",
    "            win_dict[averages_df.ix[index,'Node 1']] = win_dict[averages_df.ix[index,'Node 1']] + 1\n",
    "        else:\n",
    "            win_dict[averages_df.ix[index,'Node 1']] = 1\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if ~np.isnan(value):\n",
    "        if averages_df.ix[index,'Node 2'] in win_dict:\n",
    "            win_dict[averages_df.ix[index,'Node 2']] = win_dict[averages_df.ix[index,'Node 2']] + 1\n",
    "        else:\n",
    "            win_dict[averages_df.ix[index,'Node 2']] = 1\n",
    "\n",
    "#Counting the number of outputs for each node\n",
    "counts_dict = {}\n",
    "#Create a dict of price data\n",
    "price_dict = {'A6':20000, 'A8':12000, '3series':220000, '5series':60000, '7series':14000, 'XJ':6600, 'ES':135000, 'LS':30000, 'RX':120000, 'Sclass':25000}\n",
    "\n",
    "\n",
    "price_list = pd.Series(name='Price')\n",
    "for dictkey in price_dict:\n",
    "    price_list[dictkey] = price_dict[dictkey]\n",
    "\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if np.isnan(value):\n",
    "        continue\n",
    "    else:\n",
    "        if averages_df.ix[index,'Node 1'] in counts_dict:\n",
    "            counts_dict[averages_df.ix[index,'Node 1']] += 1\n",
    "        else:\n",
    "            counts_dict[averages_df.ix[index,'Node 1']] = 1\n",
    "\n",
    "\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if np.isnan(value):\n",
    "        continue\n",
    "    else:\n",
    "        if averages_df.ix[index,'Node 2'] in counts_dict:\n",
    "            counts_dict[averages_df.ix[index,'Node 2']] += 1\n",
    "        else:\n",
    "            counts_dict[averages_df.ix[index,'Node 2']] = 1\n",
    "\n",
    "\n",
    "\n",
    "#Creating the matrix\n",
    "A = pd.DataFrame(index=names, columns=names)\n",
    "\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if ~np.isnan(value):\n",
    "        A.ix[averages_df.ix[index,'Node 1'], averages_df.ix[index,'Node 2']] = ((wout_dict[averages_df.ix[index,'Node 2']])*win_dict[averages_df.ix[index,'Node 2']]*value)\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if ~np.isnan(value):\n",
    "        A.ix[averages_df.ix[index,'Node 2'], averages_df.ix[index,'Node 1']] = ((wout_dict[averages_df.ix[index,'Node 1']])*win_dict[averages_df.ix[index,'Node 1']]*abs(value))\n",
    "\n",
    "#weighting the values\n",
    "vector_scalar = []\n",
    "for row in range(len(A)):\n",
    "    setofmakes = []\n",
    "    for make, value in enumerate(A.ix[row,:]):\n",
    "        if ~np.isnan(value):\n",
    "            setofmakes.append(A.index[make])\n",
    "    temp_win = 0\n",
    "    temp_wout = 0\n",
    "    for i in setofmakes:\n",
    "        temp_win = win_dict[i] + temp_win\n",
    "        temp_wout = wout_dict[i] + temp_wout\n",
    "    vector_scalar.append((1.0/(temp_wout*temp_win)))\n",
    "\n",
    "A = (A.T * vector_scalar).T\n",
    "\n",
    "\n",
    "x = pd.DataFrame([1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "A.fillna(0, inplace=True)\n",
    "new_names = list(A.index.values)\n",
    "A = np.matrix(A) #Convert to Matrix\n",
    "x = np.matrix(x) #Convert to Matrix\n",
    "\n",
    "\n",
    "#Find Eigenvector/PageRank\n",
    "iterations = 30\n",
    "\n",
    "for i in range(iterations):\n",
    "    new = A*x\n",
    "    new = new/np.linalg.norm(new)\n",
    "    if np.array_equal(np.array(x),np.array(new)):\n",
    "        print 'Convergence after ', i\n",
    "        break\n",
    "    x = new\n",
    "\n",
    "#print x\n",
    "#print new\n",
    "\n",
    "newseries = pd.DataFrame(new, index=new_names)\n",
    "\n",
    "#Assign a index name to the pagerank\n",
    "final = pd.concat([newseries,price_list], axis=1)\n",
    "final.columns = ['PageRank','Sales'] \n",
    "print('Weighted Correlation')\n",
    "print(scipy.stats.spearmanr(final['PageRank'], final['Sales']))\n",
    "#print(scipy.stats.pearsonr(final['PageRank'], final['Price']))\n",
    "\n",
    "\n",
    "##Coding pagerank ----- Unweighted\n",
    "\n",
    "#Counting the number of outputs for each node\n",
    "counts_dict = {}\n",
    "\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if np.isnan(value):\n",
    "        continue\n",
    "    else:\n",
    "        if averages_df.ix[index,'Node 1'] in counts_dict:\n",
    "            counts_dict[averages_df.ix[index,'Node 1']] += 1\n",
    "        else:\n",
    "            counts_dict[averages_df.ix[index,'Node 1']] = 1\n",
    "\n",
    "\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if np.isnan(value):\n",
    "        continue\n",
    "    else:\n",
    "        if averages_df.ix[index,'Node 2'] in counts_dict:\n",
    "            counts_dict[averages_df.ix[index,'Node 2']] += 1\n",
    "        else:\n",
    "            counts_dict[averages_df.ix[index,'Node 2']] = 1\n",
    "\n",
    "\n",
    "#Creating the matrix\n",
    "A = pd.DataFrame(index=names, columns=names)\n",
    "\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if ~np.isnan(value):\n",
    "        A.ix[averages_df.ix[index,'Node 1'], averages_df.ix[index,'Node 2']] = 1.0/counts_dict[averages_df.ix[index,'Node 2']]\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if ~np.isnan(value):\n",
    "        A.ix[averages_df.ix[index,'Node 2'], averages_df.ix[index,'Node 1']] = 1.0/counts_dict[averages_df.ix[index,'Node 1']]\n",
    "\n",
    "x = pd.DataFrame([1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "A.fillna(0, inplace=True)\n",
    "new_names = list(A.index.values)\n",
    "A = np.matrix(A) #Convert to Matrix\n",
    "x = np.matrix(x) #Convert to Matrix\n",
    "\n",
    "\n",
    "#Find Eigenvector/PageRank\n",
    "iterations = 30\n",
    "\n",
    "for i in range(iterations):\n",
    "    new = A*x\n",
    "    new = new/np.linalg.norm(new)\n",
    "    if np.array_equal(np.array(x),np.array(new)):\n",
    "        print 'Convergence after ', i\n",
    "        break\n",
    "    x = new\n",
    "\n",
    "\n",
    "\n",
    "newseries = pd.DataFrame(new, index=new_names)\n",
    "\n",
    "#Assign a index name to the pagerank\n",
    "final = pd.concat([final, newseries], axis=1)\n",
    "final.columns = ['Weighted PageRank','Sales','Unweighted PageRank']\n",
    "print('Unwighted Correlation')\n",
    "print(scipy.stats.spearmanr(final['Unweighted PageRank'], final['Sales']))\n",
    "print final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part C\n",
    "Contains our code for extracting sentiment in a unique way and also all of the code from the first two parts to calculate weighted and unweighted Pagerank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########\n",
    "#Part C#\n",
    "########\n",
    "\n",
    "\n",
    "data=pd.read_csv(\"Assignment 3 Edmunds Posts.csv\", usecols=[0])\n",
    "\n",
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"LexusES\",\"ES\"))\n",
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"ES330\",\"ES\"))\n",
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"LS460\",\"LS\"))\n",
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"LS470\",\"LS\"))\n",
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"LexusLS\",\"LS\"))\n",
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"LexusRX\",\"RX\"))\n",
    "\n",
    "sw=stopwords.words('english')\n",
    "\n",
    "sw.remove('not')\n",
    "\n",
    "models_skinny=[\"es\",\"ls\",\"rx\",\"a8\",\"a6\",\"3series\",\"5series\",\"7series\",\"xj\",\"sclass\"] \n",
    "\n",
    "def tKnzr (xstring):\n",
    "    global sw\n",
    "    tokens=list(TextBlob(str(xstring)).words)\n",
    "    removeStopWords=[word for word in tokens if word.lower() not in sw]\n",
    "    lemmaed=[Word(w).lemmatize() for w in removeStopWords]\n",
    "    lowercase=[word.lower() for word in lemmaed]\n",
    "    return lowercase\n",
    "\n",
    "def finder (tokensList, modelList, numberWords):\n",
    "    blanklist=[]\n",
    "    for i in xrange(len(tokensList)):\n",
    "        if tokensList[i] in modelList:\n",
    "            blanklist.append(tokensList[i-numberWords:i+1+numberWords])\n",
    "    return blanklist\n",
    "    \n",
    "def sentiment_compiler(postseries,modelsList,numberofwords):\n",
    "    tb=Blobber(tokenizer=WordTokenizer())\n",
    "    \n",
    "    newseries=postseries.apply(tKnzr)\n",
    "    newseries=newseries.apply(lambda x: finder(x,modelsList, numberofwords))\n",
    "    newseries=newseries.apply(lambda x: [' '.join(inner) for inner in x])\n",
    "    \n",
    "    models=[\" \"+x+\" \" for x in modelsList]\n",
    "    df=pd.DataFrame(columns=modelsList).join(newseries, how=\"outer\")\n",
    "    \n",
    "    for index,l in enumerate(newseries):\n",
    "        for string in l: #looping over all the strings in model_strings\n",
    "                for model, model_skinny in zip(models,modelsList): #loop over all the models in model list\n",
    "                    if model in string: #check if model is in a particular list\n",
    "                        if isnan(df[model_skinny].iloc[index]): #correcting for neutral\n",
    "                            df[model_skinny].iloc[index]=0\n",
    "                        df[model_skinny].iloc[index]+=tb(string).sentiment[0]  \n",
    "    return df\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "rawsentiment1 = sentiment_compiler(data['Posts'],models_skinny,2)\n",
    "rawsentiment = rawsentiment1.drop(['Posts'], axis = 1)\n",
    "\n",
    "#For testing take2senti file\n",
    "#rawsentiment1 = pd.read_csv(r'C:\\Users\\beins_000\\Documents\\GitHub\\Text_Mining_MIS184N\\Group_Assignment_3\\take2senti.csv')\n",
    "#rawsentiment = rawsentiment1.drop('Unnamed: 0', axis=1) #Testing bejans\n",
    "\n",
    "lab_for_C = ['ES', 'LS', 'RX', 'A8', 'A6', '3series', '5series', '7series', 'XJ', 'Sclass']\n",
    "rawsentiment.columns = lab_for_C\n",
    "rawsentiment = rawsentiment * 5\n",
    "\n",
    "\n",
    "#names = ['ES-LS','ES-RX','ES-A8','ES-A6','ES-3series','ES-5series','ES-7series','ES-XJ','ES-Sclass','LS-RX','LS-A8','LS-A6','LS-3series','LS-5series','LS-7series','LS-XJ','LS-Sclass','RX-A8','RX-A6','RX-3series','RX-5series','RX-7series','RX-XJ','RX-Sclass','A8-A6','A8-3series','A8-5series','A8-7series','A8-XJ','A8-Sclass','A6-3series','A6-5series','A6-7series','A6-XJ','A6-Sclass','3series-5series','3series-7series','3series-XJ','3series-Sclass','5series-7series','5series-XJ','5series-Sclass','7series-XJ','7series-Sclass','XJ-Sclass']\n",
    "names = list(rawsentiment.columns)\n",
    "#i=1\n",
    "#j=1\n",
    "\n",
    "##Create the sentiment difference dataframe (ES-LS, ES-RX, etc)\n",
    "length_senti = len(rawsentiment)\n",
    "width_senti = len(rawsentiment.T)\n",
    "comparisons = pd.DataFrame(index=np.array(range(length_senti)))\n",
    "#Vectors for future use in the nodes\n",
    "node_name_1 = []\n",
    "node_name_2 = []\n",
    "\n",
    "n=1 #For counter\n",
    "for i in range(width_senti-1):\n",
    "    for j in range(width_senti - n):\n",
    "        comparisons[str(names[i]) + '-' + str(names[(j+n)])] = np.array(rawsentiment[[i]]) - np.array(rawsentiment[[j+n]])\n",
    "        node_name_1.append(names[i])\n",
    "        node_name_2.append(names[(j+n)])\n",
    "    n=n+1\n",
    "\n",
    "length_comp = len(comparisons.T)\n",
    "\n",
    "averages_df = pd.DataFrame(index=comparisons.columns, columns = ['Positive','Negative'])\n",
    "\n",
    "\n",
    "\n",
    "for i in comparisons.columns:\n",
    "    pos_sum = 0\n",
    "    neg_sum = 0    \n",
    "    pos_sum = [pos_sum + x for x in comparisons[i] if x > 0]\n",
    "    neg_sum = [neg_sum + x for x in comparisons[i] if x < 0]\n",
    "    if len(pos_sum) != 0:    \n",
    "        averages_df.loc[i,'Positive'] = sum(pos_sum)/len(pos_sum)\n",
    "    if len(neg_sum) != 0:\n",
    "        averages_df.loc[i,'Negative'] = sum(neg_sum)/len(neg_sum)\n",
    "\n",
    "averages_df['Node 1'] = node_name_1\n",
    "averages_df['Node 2'] = node_name_2\n",
    "\n",
    "##Creating the network graph\n",
    "G=nx.DiGraph()\n",
    "\n",
    "#Creating the labels for the nodes and adding a node to the G object\n",
    "labels = {}\n",
    "opp_labels = {}\n",
    "for i, value in enumerate(names):\n",
    "    labels[i] = str(value)\n",
    "    opp_labels[value] = i\n",
    "    G.add_node(i) #Adding the nodes\n",
    "    \n",
    "\n",
    "#Create the list of tuples for edges\n",
    "node_list = []\n",
    "for index, pref in enumerate(averages_df.loc[:,'Positive']): #Positives\n",
    "    if ~np.isnan(pref):\n",
    "        node_list.append((pd.Series(averages_df.loc[:,'Node 2'])[index],pd.Series(averages_df.loc[:,'Node 1'])[index]))\n",
    "        G.add_edge(*(opp_labels[pd.Series(averages_df.loc[:,'Node 2'])[index]],opp_labels[pd.Series(averages_df.loc[:,'Node 1'])[index]]))\n",
    "for index, pref in enumerate(averages_df.loc[:,'Negative']): #Negatives\n",
    "    if ~np.isnan(pref):\n",
    "        node_list.append((pd.Series(averages_df.loc[:,'Node 1'])[index],pd.Series(averages_df.loc[:,'Node 2'])[index]))\n",
    "        G.add_edge(*(opp_labels[pd.Series(averages_df.loc[:,'Node 1'])[index]],opp_labels[pd.Series(averages_df.loc[:,'Node 2'])[index]]))\n",
    "\n",
    "\n",
    "pos = nx.circular_layout(G) #Positions\n",
    "\n",
    "#nx.edges([(1,2),(1,0)])#node_list)\n",
    "\n",
    "nx.draw_networkx_labels(G,pos, labels, font_size=16)\n",
    "#nx.draw_networkx_nodes(G,pos,node_color='skyblue')\n",
    "nx.draw(G,pos,node_size=1000)\n",
    "###nx.Graph(node_list)\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "plt.figure(num=None, figsize=(30,20))\n",
    "#plt.savefig('network.png', dpi=100)\n",
    "\n",
    "plt.show()\n",
    "fig1.savefig('network.png', dpi=1000)\n",
    "\n",
    "\n",
    "##Coding pagerank ------- Weighted\n",
    "\n",
    "\n",
    "#Weights out\n",
    "wout_dict = {}\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if ~np.isnan(value):\n",
    "        if averages_df.ix[index,'Node 2'] in wout_dict:\n",
    "            wout_dict[averages_df.ix[index,'Node 2']] = wout_dict[averages_df.ix[index,'Node 2']] + 1\n",
    "        else:\n",
    "            wout_dict[averages_df.ix[index,'Node 2']] = 1\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if ~np.isnan(value):\n",
    "        if averages_df.ix[index,'Node 1'] in wout_dict:\n",
    "            wout_dict[averages_df.ix[index,'Node 1']] = wout_dict[averages_df.ix[index,'Node 1']] + 1\n",
    "        else:\n",
    "            wout_dict[averages_df.ix[index,'Node 1']] = 1\n",
    "\n",
    "#Weights in\n",
    "win_dict = {}\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if ~np.isnan(value):\n",
    "        if averages_df.ix[index,'Node 1'] in win_dict:\n",
    "            win_dict[averages_df.ix[index,'Node 1']] = win_dict[averages_df.ix[index,'Node 1']] + 1\n",
    "        else:\n",
    "            win_dict[averages_df.ix[index,'Node 1']] = 1\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if ~np.isnan(value):\n",
    "        if averages_df.ix[index,'Node 2'] in win_dict:\n",
    "            win_dict[averages_df.ix[index,'Node 2']] = win_dict[averages_df.ix[index,'Node 2']] + 1\n",
    "        else:\n",
    "            win_dict[averages_df.ix[index,'Node 2']] = 1\n",
    "\n",
    "#Counting the number of outputs for each node\n",
    "counts_dict = {}\n",
    "#Create a dict of price data\n",
    "price_dict = {'A6':20000, 'A8':12000, '3series':220000, '5series':60000, '7series':14000, 'XJ':6600, 'ES':135000, 'LS':30000, 'RX':120000, 'Sclass':25000}\n",
    "\n",
    "\n",
    "price_list = pd.Series(name='Price')\n",
    "for dictkey in price_dict:\n",
    "    price_list[dictkey] = price_dict[dictkey]\n",
    "\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if np.isnan(value):\n",
    "        continue\n",
    "    else:\n",
    "        if averages_df.ix[index,'Node 1'] in counts_dict:\n",
    "            counts_dict[averages_df.ix[index,'Node 1']] += 1\n",
    "        else:\n",
    "            counts_dict[averages_df.ix[index,'Node 1']] = 1\n",
    "\n",
    "\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if np.isnan(value):\n",
    "        continue\n",
    "    else:\n",
    "        if averages_df.ix[index,'Node 2'] in counts_dict:\n",
    "            counts_dict[averages_df.ix[index,'Node 2']] += 1\n",
    "        else:\n",
    "            counts_dict[averages_df.ix[index,'Node 2']] = 1\n",
    "\n",
    "\n",
    "\n",
    "#Creating the matrix\n",
    "A = pd.DataFrame(index=names, columns=names)\n",
    "\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if ~np.isnan(value):\n",
    "        A.ix[averages_df.ix[index,'Node 1'], averages_df.ix[index,'Node 2']] = ((wout_dict[averages_df.ix[index,'Node 2']])*win_dict[averages_df.ix[index,'Node 2']]*value)\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if ~np.isnan(value):\n",
    "        A.ix[averages_df.ix[index,'Node 2'], averages_df.ix[index,'Node 1']] = ((wout_dict[averages_df.ix[index,'Node 1']])*win_dict[averages_df.ix[index,'Node 1']]*abs(value))\n",
    "\n",
    "#weighting the values\n",
    "vector_scalar = []\n",
    "for row in range(len(A)):\n",
    "    setofmakes = []\n",
    "    for make, value in enumerate(A.ix[row,:]):\n",
    "        if ~np.isnan(value):\n",
    "            setofmakes.append(A.index[make])\n",
    "    temp_win = 0\n",
    "    temp_wout = 0\n",
    "    for i in setofmakes:\n",
    "        temp_win = win_dict[i] + temp_win\n",
    "        temp_wout = wout_dict[i] + temp_wout\n",
    "    vector_scalar.append((1.0/(temp_wout*temp_win)))\n",
    "\n",
    "A = (A.T * vector_scalar).T\n",
    "\n",
    "\n",
    "x = pd.DataFrame([1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "A.fillna(0, inplace=True)\n",
    "new_names = list(A.index.values)\n",
    "A = np.matrix(A) #Convert to Matrix\n",
    "x = np.matrix(x) #Convert to Matrix\n",
    "\n",
    "\n",
    "#Find Eigenvector/PageRank\n",
    "iterations = 30\n",
    "\n",
    "for i in range(iterations):\n",
    "    new = A*x\n",
    "    new = new/np.linalg.norm(new)\n",
    "    if np.array_equal(np.array(x),np.array(new)):\n",
    "        print 'Convergence after ', i\n",
    "        break\n",
    "    x = new\n",
    "\n",
    "#print x\n",
    "#print new\n",
    "\n",
    "newseries = pd.DataFrame(new, index=new_names)\n",
    "\n",
    "#Assign a index name to the pagerank\n",
    "final = pd.concat([newseries,price_list], axis=1)\n",
    "final.columns = ['PageRank','Sales'] \n",
    "print('Weighted Correlation')\n",
    "print(scipy.stats.spearmanr(final['PageRank'], final['Sales']))\n",
    "#print(scipy.stats.pearsonr(final['PageRank'], final['Price']))\n",
    "\n",
    "\n",
    "##Coding pagerank ----- Unweighted\n",
    "\n",
    "#Counting the number of outputs for each node\n",
    "counts_dict = {}\n",
    "\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if np.isnan(value):\n",
    "        continue\n",
    "    else:\n",
    "        if averages_df.ix[index,'Node 1'] in counts_dict:\n",
    "            counts_dict[averages_df.ix[index,'Node 1']] += 1\n",
    "        else:\n",
    "            counts_dict[averages_df.ix[index,'Node 1']] = 1\n",
    "\n",
    "\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if np.isnan(value):\n",
    "        continue\n",
    "    else:\n",
    "        if averages_df.ix[index,'Node 2'] in counts_dict:\n",
    "            counts_dict[averages_df.ix[index,'Node 2']] += 1\n",
    "        else:\n",
    "            counts_dict[averages_df.ix[index,'Node 2']] = 1\n",
    "\n",
    "\n",
    "#Creating the matrix\n",
    "A = pd.DataFrame(index=names, columns=names)\n",
    "\n",
    "for index, value in enumerate(averages_df.ix[:,'Positive']):\n",
    "    if ~np.isnan(value):\n",
    "        A.ix[averages_df.ix[index,'Node 1'], averages_df.ix[index,'Node 2']] = 1.0/counts_dict[averages_df.ix[index,'Node 2']]\n",
    "for index, value in enumerate(averages_df.ix[:,'Negative']):\n",
    "    if ~np.isnan(value):\n",
    "        A.ix[averages_df.ix[index,'Node 2'], averages_df.ix[index,'Node 1']] = 1.0/counts_dict[averages_df.ix[index,'Node 1']]\n",
    "\n",
    "x = pd.DataFrame([1,1,1,1,1,1,1,1,1,1])\n",
    "\n",
    "A.fillna(0, inplace=True)\n",
    "new_names = list(A.index.values)\n",
    "A = np.matrix(A) #Convert to Matrix\n",
    "x = np.matrix(x) #Convert to Matrix\n",
    "\n",
    "\n",
    "#Find Eigenvector/PageRank\n",
    "iterations = 30\n",
    "\n",
    "for i in range(iterations):\n",
    "    new = A*x\n",
    "    new = new/np.linalg.norm(new)\n",
    "    if np.array_equal(np.array(x),np.array(new)):\n",
    "        print 'Convergence after ', i\n",
    "        break\n",
    "    x = new\n",
    "\n",
    "\n",
    "\n",
    "newseries = pd.DataFrame(new, index=new_names)\n",
    "\n",
    "#Assign a index name to the pagerank\n",
    "final = pd.concat([final, newseries], axis=1)\n",
    "final.columns = ['Weighted PageRank','Sales','Unweighted PageRank']\n",
    "print('Unwighted Correlation')\n",
    "print(scipy.stats.spearmanr(final['Unweighted PageRank'], final['Sales']))\n",
    "print final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
