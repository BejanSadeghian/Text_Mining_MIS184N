{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "from numpy import isnan\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from textblob import Word\n",
    "from textblob.base import BaseTokenizer\n",
    "from textblob import TextBlob\n",
    "from textblob import Blobber\n",
    "from textblob.tokenizers import WordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Assignment 3 Edmunds Posts.csv\", usecols=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"LexusES\",\"ES\"))\n",
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"LexusLS\",\"LS\"))\n",
    "data['Posts'] = data['Posts'].map(lambda x: x.replace(\"LexusRX\",\"RX\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sw=stopwords.words('english')\n",
    "\n",
    "sw.remove('not')\n",
    "\n",
    "models_skinny=[\"es\",\"ls\",\"rx\",\"a8\",\"a6\",\"3series\",\\\n",
    "               \"5series\",\"7series\",\"xj\",\"sclass\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' es ',\n",
       " ' ls ',\n",
       " ' rx ',\n",
       " ' a8 ',\n",
       " ' a6 ',\n",
       " ' 3series ',\n",
       " ' 5series ',\n",
       " ' 7series ',\n",
       " ' xj ',\n",
       " ' sclass ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models=[\" \"+x+\" \" for x in models_skinny]\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tKnzr (xstring):\n",
    "    global sw\n",
    "    tokens=list(TextBlob(str(xstring)).words)\n",
    "    removeStopWords=[word for word in tokens if word.lower() not in sw]\n",
    "    lemmaed=[Word(w).lemmatize() for w in removeStopWords]\n",
    "    lowercase=[word.lower() for word in lemmaed]\n",
    "    return lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finder (tokensList, modelList, numberWords):\n",
    "    blanklist=[]\n",
    "    for i in xrange(len(tokensList)):\n",
    "        if tokensList[i] in modelList:\n",
    "            blanklist.append(tokensList[i-numberWords:i+1+numberWords])\n",
    "    return blanklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filling in the data matrix\n",
    "\n",
    "for index,l in enumerate(df['model_strings']):\n",
    "    for string in l: #looping over all the strings in model_strings\n",
    "            for model in models: #loop over all the models in model list\n",
    "                if model in string: #check if model is in a particular list\n",
    "                    if isnan(df[model].iloc[index]): #correcting for neutral\n",
    "                        df[model].iloc[index]=0\n",
    "                    df[model].iloc[index]+=tb(string).sentiment[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment_compiler(postseries,modelsList,numberofwords):\n",
    "    newseries=postseries.apply(tKnzr)\n",
    "    newseries=newseries.apply(lambda x: finder(x,modelsList, numberofwords))\n",
    "    newseries=newseries.apply(lambda x: [' '.join(inner) for inner in x])\n",
    "    models=[\" \"+x+\" \" for x in modelsList]\n",
    "    df=pd.DataFrame(columns=modelsList).join(newseries, how=\"outer\")\n",
    "    for index,l in enumerate(newseries):\n",
    "        for string in l: #looping over all the strings in model_strings\n",
    "                for model, model_skinny in zip(models,modelsList): #loop over all the models in model list\n",
    "                    if model in string: #check if model is in a particular list\n",
    "                        if isnan(df[model_skinny].iloc[index]): #correcting for neutral\n",
    "                            df[model_skinny].iloc[index]=0\n",
    "                        df[model_skinny].iloc[index]+=tb(string).sentiment[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'tb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a9a822b4705a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msentiment_compiler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Posts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodels_skinny\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-e93722979f26>\u001b[0m in \u001b[0;36msentiment_compiler\u001b[1;34m(postseries, modelsList, numberofwords)\u001b[0m\n\u001b[0;32m     11\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_skinny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#correcting for neutral\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                             \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_skinny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_skinny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: global name 'tb' is not defined"
     ]
    }
   ],
   "source": [
    "sentiment_compiler(data['Posts'],models_skinny,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "notify_time": "0"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
