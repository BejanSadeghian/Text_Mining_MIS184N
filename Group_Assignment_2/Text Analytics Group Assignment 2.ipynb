{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 83,
=======
   "execution_count": 3,
>>>>>>> Stashed changes
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "from patsy import dmatrices\n",
    "from sklearn.cross_validation import train_test_split,StratifiedShuffleSplit,StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.cross_validation import \n",
    "#from sklearn.cross_validation import \n",
    "import scipy.sparse\n",
    "import sklearn.cluster\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp = pd.read_csv(\"yelp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_mask = yelp['stars'] > 3\n",
    "yelp['High'] = 0\n",
    "yelp.ix[high_mask, 'High'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Task A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formula = 'High ~ 0 + votes_cool + votes_funny + votes_useful + Cheap + Moderate + Expensive  ' + \\\n",
    "' + VeryExpensive + American + Chinese + French + Japanese + Indian + Italian + Greek ' + \\\n",
    "' + Mediterranean + Mexican + Thai + Vietnamese + Others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y, X = dmatrices(formula, yelp, return_type='dataframe')\n",
    "y = Y['High'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = StratifiedShuffleSplit(y, n_iter = 1, test_size = 0.3, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StratifiedShuffleSplit(labels=[ 0.  0.  1. ...,  1.  0.  1.], n_iter=1, test_size=0.3, random_state=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DF_X = DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11930  5364 16234 ...,  3282 16218  3490]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in index:\n",
    "    print(train_index)\n",
    "    X_train, X_test = DF_X.iloc[train_index,], DF_X.iloc[test_index,]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>votes_cool</th>\n",
       "      <th>votes_funny</th>\n",
       "      <th>votes_useful</th>\n",
       "      <th>Cheap</th>\n",
       "      <th>Moderate</th>\n",
       "      <th>Expensive</th>\n",
       "      <th>VeryExpensive</th>\n",
       "      <th>American</th>\n",
       "      <th>Chinese</th>\n",
       "      <th>French</th>\n",
       "      <th>Japanese</th>\n",
       "      <th>Indian</th>\n",
       "      <th>Italian</th>\n",
       "      <th>Greek</th>\n",
       "      <th>Mediterranean</th>\n",
       "      <th>Mexican</th>\n",
       "      <th>Thai</th>\n",
       "      <th>Vietnamese</th>\n",
       "      <th>Others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11930</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5364</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16234</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7821</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       votes_cool  votes_funny  votes_useful  Cheap  Moderate  Expensive  \\\n",
       "11930           0            0             0      1         0          0   \n",
       "5364            0            0             1      0         1          0   \n",
       "16234           2            1             3      0         1          0   \n",
       "7821            0            0             0      0         1          0   \n",
       "19994           3            1             4      0         0          1   \n",
       "\n",
       "       VeryExpensive  American  Chinese  French  Japanese  Indian  Italian  \\\n",
       "11930              0         0        0       0         0       0        0   \n",
       "5364               0         0        0       0         0       0        1   \n",
       "16234              0         1        0       0         0       0        0   \n",
       "7821               0         0        0       0         0       0        0   \n",
       "19994              0         0        0       0         0       0        0   \n",
       "\n",
       "       Greek  Mediterranean  Mexican  Thai  Vietnamese  Others  \n",
       "11930      0              0        1     0           0       0  \n",
       "5364       0              0        0     0           0       0  \n",
       "16234      0              0        0     0           0       0  \n",
       "7821       0              0        0     0           0       1  \n",
       "19994      0              0        0     0           0       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_model = LogisticRegression()\n",
    "logistic_result = logistic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.68547753411\n"
     ]
    }
   ],
   "source": [
    "logistic_train_prediction = logistic_model.predict(X_train)\n",
    "print metrics.accuracy_score(y_train, logistic_train_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6805\n"
     ]
    }
   ],
   "source": [
    "logistic_test_prediction = logistic_model.predict(X_test)\n",
    "print metrics.accuracy_score(y_test, logistic_test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_model = neighbors.KNeighborsClassifier(n_neighbors=15, weights='uniform', p=2)\n",
    "knn_result = knn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693263804557\n"
     ]
    }
   ],
   "source": [
    "knn_train_prediction = knn_model.predict(X_train)\n",
    "print metrics.accuracy_score(y_train, knn_train_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.664666666667\n"
     ]
    }
   ],
   "source": [
    "knn_test_prediction = knn_model.predict(X_test)\n",
    "print metrics.accuracy_score(y_test, knn_test_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Task B: Classification on Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234567)\n",
    "train = yelp.sample(int(len(yelp)*0.7), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = yelp[~yelp.index.isin(train.index.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train['Review']\n",
    "train_y = train['High']\n",
    "test_x = test['Review']\n",
    "test_y = test['High']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0,smooth_idf=True, strip_accents='unicode', norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_classification (v):\n",
    "    X_transform=v.fit_transform(train_x)\n",
    "    X_test=v.transform(test_x)\n",
    "    \n",
    "    nb_classifier = MultinomialNB().fit(X_transform, train_y)\n",
    "    y_nb_predicted = nb_classifier.predict(X_test)\n",
    "    \n",
    "    predict_y=Series(y_nb_predicted).reset_index()[0]\n",
    "    df=pd.DataFrame()\n",
    "    df['Predicted']=predict_y\n",
    "    df['Actual']=test_y.reset_index()['High']\n",
    "    \n",
    "    print \"Percent Correct\\n\",round((df['Predicted']==df['Actual']).mean()*100,3)\n",
    "    print \"\\nConfusion Matrix\\n\",pd.crosstab(index=df['Actual'],columns=df['Predicted'])\n",
    "    print \"\\nProportion Table\\n\", pd.crosstab(index=df['Actual'],columns=df['Predicted']).apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Correct\n",
      "69.267\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted   0     1\n",
      "Actual             \n",
      "0          73  1842\n",
      "1           2  4083\n",
      "\n",
      "Proportion Table\n",
      "Predicted        0        1\n",
      "Actual                     \n",
      "0          0.03812  0.96188\n",
      "1          0.00049  0.99951\n"
     ]
    }
   ],
   "source": [
    "text_classification(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Undersampling data set to ensure 50/50 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "highs=yelp[yelp['High']==1]\n",
    "lows=yelp[yelp['High']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_high=highs.sample(len(lows),replace=False).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample=sample_high.append(lows, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=sample.sample(int(0.7*len(sample)),replace=False).copy()\n",
    "test=sample[~sample.index.isin(train.index.values)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train['Review']\n",
    "train_y = train['High']\n",
    "test_x = test['Review']\n",
    "test_y = test['High']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Correct\n",
      "82.678\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted     0     1\n",
      "Actual               \n",
      "0          1654   289\n",
      "1           381  1544\n",
      "\n",
      "Proportion Table\n",
      "Predicted         0         1\n",
      "Actual                       \n",
      "0          0.851261  0.148739\n",
      "1          0.197922  0.802078\n"
     ]
    }
   ],
   "source": [
    "text_classification(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Task C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def yelper(removeList,sample=sample):\n",
    "    train=sample.sample(int(0.7*len(sample)),replace=False).copy()\n",
    "    test=sample[~sample.index.isin(train.index.values)].copy()\n",
    "    \n",
    "    train_x = train['Review']\n",
    "    train_y = train['High']\n",
    "    test_x = test['Review']\n",
    "    test_y = test['High']\n",
    "    \n",
    "    X_transform=vectorizer.fit_transform(train_x) #just the reviews\n",
    "    sam=train.drop(removeList, axis=1).copy()\n",
    "    samsparse=scipy.sparse.csr_matrix(sam.to_sparse()) #sparsing the numeric\n",
    "    surprise=scipy.sparse.hstack([X_transform, samsparse]) #combining numeric and text\n",
    "    \n",
    "    X_test=vectorizer.transform(test_x) #repeating above for test\n",
    "    samt=test.drop(removeList, axis=1).copy() #removing irrelevant\n",
    "    samsparset=scipy.sparse.csr_matrix(samt.to_sparse()) #sparsing numeric \n",
    "    surpriset=scipy.sparse.hstack([X_test, samsparset]) #combining numeric and text\n",
    "    \n",
    "    nb_classifier = MultinomialNB().fit(surprise, train_y)\n",
    "    y_nb_predicted = nb_classifier.predict(surpriset)\n",
    "    \n",
    "    predict_y=Series(y_nb_predicted).reset_index()[0]\n",
    "    df=pd.DataFrame()\n",
    "    df['Predicted']=predict_y\n",
    "    df['Actual']=test_y.reset_index()['High']\n",
    "    \n",
    "    print \"Percent Correct\\n\",round((df['Predicted']==df['Actual']).mean()*100,3)\n",
    "    print \"\\nConfusion Matrix\\n\",pd.crosstab(index=df['Actual'],columns=df['Predicted'])\n",
    "    print \"\\nProportion Table\\n\", pd.crosstab(index=df['Actual'],columns=df['Predicted']).apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Correct\n",
      "80.274\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted     0     1\n",
      "Actual               \n",
      "0          1526   427\n",
      "1           336  1579\n",
      "\n",
      "Proportion Table\n",
      "Predicted         0         1\n",
      "Actual                       \n",
      "0          0.781362  0.218638\n",
      "1          0.175457  0.824543\n"
     ]
    }
   ],
   "source": [
    "rL=[\"Review\",\"stars\",\"High\"]\n",
    "yelper(rL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(sample['High'], n_folds=3, random_state=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawsentiment = pd.read_excel('Yelp_Review_Data_Results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawsentiment['True_Sentiment'] = rawsentiment['Pos Senti'] + rawsentiment['Neg Senti']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawsentiment['Actual'] = yelp['High']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Raw Sentiment, 0 defaults to low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Correct\n",
      "58.471\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted     0     1\n",
      "Actual               \n",
      "0          2354  4092\n",
      "1          4213  9339\n",
      "\n",
      "Proportion Table\n",
      "Predicted         0         1\n",
      "Actual                       \n",
      "0          0.365188  0.634812\n",
      "1          0.310877  0.689123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beins_000\\Anaconda\\lib\\site-packages\\pandas\\core\\indexing.py:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "rawsentiment['Predicted']=0\n",
    "rawsentiment['Predicted'].ix[rawsentiment['True_Sentiment']>0]=1\n",
    "df=rawsentiment\n",
    "print \"Percent Correct\\n\",round((df['Predicted']==df['Actual']).mean()*100,3)\n",
    "print \"\\nConfusion Matrix\\n\",pd.crosstab(index=df['Actual'],columns=df['Predicted'])\n",
    "print \"\\nProportion Table\\n\", pd.crosstab(index=df['Actual'],columns=df['Predicted']).apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Raw Sentiment, 0 defaults to high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Correct\n",
      "35.914\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted      0     1\n",
      "Actual                \n",
      "0           5457   989\n",
      "1          11827  1725\n",
      "\n",
      "Proportion Table\n",
      "Predicted         0         1\n",
      "Actual                       \n",
      "0          0.846572  0.153428\n",
      "1          0.872713  0.127287\n"
     ]
    }
   ],
   "source": [
    "rawsentiment['Predicted']=0\n",
    "rawsentiment['Predicted'].ix[rawsentiment['True_Sentiment']<0]=1\n",
    "df=rawsentiment\n",
    "print \"Percent Correct\\n\",round((df['Predicted']==df['Actual']).mean()*100,3)\n",
    "print \"\\nConfusion Matrix\\n\",pd.crosstab(index=df['Actual'],columns=df['Predicted'])\n",
    "print \"\\nProportion Table\\n\", pd.crosstab(index=df['Actual'],columns=df['Predicted']).apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DTMReviews = vectorizer.fit_transform(sample['Review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "#def new_euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False):\n",
    "#    return cosine_similarity(X,Y)\n",
    "\n",
    "# monkey patch (ensure cosine dist function is used)\n",
    "#from sklearn.cluster import k_means_k_means_.euclidean_distances\n",
    "#k_means_.euclidean_distances = new_euclidean_distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cluster = sklearn.cluster.KMeans(n_clusters=2, random_state = 1)\n",
    "clusterout = Cluster.fit(DTMReviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "series_clusters = Series(clusterout.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample['Cluster'] = series_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.concat([sample, series_clusters], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>votes_cool</th>\n",
       "      <th>votes_funny</th>\n",
       "      <th>votes_useful</th>\n",
       "      <th>Cheap</th>\n",
       "      <th>Moderate</th>\n",
       "      <th>Expensive</th>\n",
       "      <th>VeryExpensive</th>\n",
       "      <th>American</th>\n",
       "      <th>Chinese</th>\n",
       "      <th>...</th>\n",
       "      <th>Italian</th>\n",
       "      <th>Greek</th>\n",
       "      <th>Mediterranean</th>\n",
       "      <th>Mexican</th>\n",
       "      <th>Thai</th>\n",
       "      <th>Vietnamese</th>\n",
       "      <th>Others</th>\n",
       "      <th>Review</th>\n",
       "      <th>High</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>This place is awesome  and I m not even a vege...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Get the ribs and go home fat and happy! Also  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Best Italian food I ve ever had.  They got us ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I ordered one of China Chili s more pedestrian...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Lean and green was too damn yummy to be on the...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars  votes_cool  votes_funny  votes_useful  Cheap  Moderate  Expensive  \\\n",
       "0      4           0            0             0      0         1          0   \n",
       "1      4           1            2             2      0         1          0   \n",
       "2      5           0            0             0      0         1          0   \n",
       "3      4           1            0             1      0         1          0   \n",
       "4      4           0            0             1      0         1          0   \n",
       "\n",
       "   VeryExpensive  American  Chinese   ...     Italian  Greek  Mediterranean  \\\n",
       "0              0         0        0   ...           0      0              0   \n",
       "1              0         1        0   ...           0      0              0   \n",
       "2              0         0        0   ...           1      0              0   \n",
       "3              0         0        1   ...           0      0              0   \n",
       "4              0         1        0   ...           0      0              0   \n",
       "\n",
       "   Mexican  Thai  Vietnamese  Others  \\\n",
       "0        0     0           0       1   \n",
       "1        0     0           0       0   \n",
       "2        0     0           0       0   \n",
       "3        0     0           0       0   \n",
       "4        0     0           0       0   \n",
       "\n",
       "                                              Review  High  Cluster  \n",
       "0  This place is awesome  and I m not even a vege...     1        1  \n",
       "1  Get the ribs and go home fat and happy! Also  ...     1        1  \n",
       "2  Best Italian food I ve ever had.  They got us ...     1        1  \n",
       "3  I ordered one of China Chili s more pedestrian...     1        0  \n",
       "4  Lean and green was too damn yummy to be on the...     1        1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix \n",
      "Cluster     0     1\n",
      "High               \n",
      "0        3456  2990\n",
      "1        2351  4095\n",
      "\n",
      "Percent Correct (if cluster 1 = low)\n",
      "41.429\n",
      "\n",
      "Percent Correct (if cluster 1 = high)\n",
      "58.571\n",
      "\n",
      "Proportion Table\n",
      "Cluster         0         1\n",
      "High                       \n",
      "0        0.536146  0.463854\n",
      "1        0.364722  0.635278\n"
     ]
    }
   ],
   "source": [
    "print \"Confusion Matrix \\n\", pd.crosstab(index=sample['High'],columns=sample['Cluster'])\n",
    "print \"\\nPercent Correct (if cluster 1 = low)\\n\", round((sample['High']!=sample['Cluster']).mean()*100,3)\n",
    "print \"\\nPercent Correct (if cluster 1 = high)\\n\", round((sample['High']==sample['Cluster']).mean()*100,3)\n",
    "print \"\\nProportion Table\\n\", pd.crosstab(index=sample['High'],columns=sample['Cluster']).apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part F"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from nltk import download\n",
    "#from textblob_aptagger import PerceptronTagger\n",
    "#from textblob import Blobber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews=yelp['Review']\n",
    "reviews=reviews.str.decode(\"utf-8\")\n",
    "reviews_high = reviews[yelp['High'] == 1].copy()\n",
    "reviews_low = reviews[yelp['High'] == 0].copy()\n",
    "reviews=list(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_high = reviews_high.map(word_tokenize)\n",
    "token_low = reviews_low.map(word_tokenize)\n",
    "token_high = list(token_high)\n",
    "token_low = list(token_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flat_high = list(itertools.chain.from_iterable(token_high))\n",
    "flat_low = list(itertools.chain.from_iterable(token_low))\n",
    "high_lower = [t.lower() for t in flat_high if t.isalpha()]\n",
    "low_lower = [t.lower() for t in flat_low if t.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_vc = Series(high_lower).value_counts()\n",
    "low_vc = Series(low_lower).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_clean = [word for word in high_vc.index if word not in stopwords.words('english')]\n",
    "low_clean = [word for word in low_vc.index if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag_high100 = pos_tag(high_clean[:100])\n",
    "tag_low100 = pos_tag(low_clean[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_noun = [word for word,tag in tag_high100 if tag == 'NN']\n",
    "low_noun = [word for word,tag in tag_low100 if tag == 'NN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'food', u'place', u'service', u'menu', u'restaurant', u'try', u'm', u'nice', u'lunch', u'order', u'cheese', u'phoenix', u'eat', u'went', u'make', u'think', u'staff', u'tasty', u'bar', u'wait', u'pizza', u're', u'way', u'burger', u'll', u'bit', u'awesome', u'excellent', u'come', u'didn']\n"
     ]
    }
   ],
   "source": [
    "print high_noun[:30] #Food, Place/Atmosphere, Service, Menu, Wait Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'food', u'place', u'service', u'restaurant', u'order', u'm', u'nice', u'think', u'sauce', u'table', u'try', u'cheese', u'way', u'eat', u'wasn', u'nothing', u'something', u'want', u'bit', u'night', u'side', u'd', u'mexican', u'come', u'bar', u'll', u'ok', u'rice', u'flavor', u'experience']\n"
     ]
    }
   ],
=======
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "print low_noun[:30] #Food, Place/Atmosphere, Service, Order, Cleanliness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def noun_extractor(series):\n",
    "    token = series.map(word_tokenize)\n",
    "    tag = token.map(pos_tag)\n",
    "    [x for x in tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_of_hw = sum(high_vc)\n",
    "number_of_lw = sum(low_vc)\n",
    "mask = (high_vc > 10) | (low_vc > 10)\n",
    "\n",
    "high_vc_norm = (1+high_vc)/number_of_hw\n",
    "low_vc_norm = (1+low_vc)/number_of_lw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Task F (extra): Finding Top Tokens Predictive of High/Low Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat_vc = pd.concat([high_vc,low_vc],join='outer', axis=1).fillna(0)\n",
    "concat_vc.rename(columns={0:'High',1:'Low'}, inplace=True)\n",
    "mask = (concat_vc['High'] > 10) | (concat_vc['Low'] > 10)\n",
    "concat_vc = concat_vc[mask]\n",
    "\n",
    "high_vc_norm = (1+concat_vc['High'])/number_of_hw\n",
    "low_vc_norm = (1+concat_vc['Low'])/number_of_lw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Concat two series\n",
    "concat_high_low = pd.concat([high_vc_norm,low_vc_norm],join='outer', axis=1)\n",
    "concat_high_low.rename(columns={0:'High',1:'Low'}, inplace=True)\n",
    "concat_high_low['High'].fillna(0.000001, inplace=True)\n",
    "concat_high_low['Low'].fillna(0.000001, inplace=True)\n",
    "concat_high_low['HL_Ratio']= (concat_high_low['High'].map(math.log)-concat_high_low['Low'].map(math.log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "concat_high_low.sort('HL_Ratio', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "low_100ratio = concat_high_low[:100]\n",
    "high_100ratio = concat_high_low[-100:]\n",
    "low_clean_2 = [word for word in low_100ratio.index if word not in stopwords.words('english')]\n",
    "high_clean_2 = [word for word in high_100ratio.index if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_high100_2 = pos_tag(high_clean_2[:100])\n",
    "tag_low100_2 = pos_tag(low_clean_2[:100])\n",
    "high_noun_2 = [word for word,tag in tag_high100_2 if tag == 'NN']\n",
    "low_noun_2 = [word for word,tag in tag_low100_2 if tag == 'NN']"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 145,
=======
   "execution_count": 6,
>>>>>>> Stashed changes
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'blandest',\n",
       " u'refund',\n",
       " u'downhill',\n",
       " u'rotten',\n",
       " u'indifferent',\n",
       " u'meager',\n",
       " u'flavorless',\n",
       " u'abe',\n",
       " u'subpar',\n",
       " u'worst']"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 145,
=======
     "execution_count": 6,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_noun_2[:10]"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 146,
=======
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PerceptronTagger())\n",
    "tags = [tb(blob).tags for blob in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tag_series=Series(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.concat([yelp['High'],tag_series], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
>>>>>>> Stashed changes
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
<<<<<<< Updated upstream
      "text/plain": [
       "[u'lifetime',\n",
       " u'encourage',\n",
       " u'confit',\n",
       " u'virgin',\n",
       " u'pistachio',\n",
       " u'lori',\n",
       " u'rocket',\n",
       " u'crema',\n",
       " u'ecuadorian',\n",
       " u'gabi']"
      ]
     },
     "execution_count": 146,
=======
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[(This, DT), (location, NN), (is, VBZ), (out, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[(=, SYM), (=, FW), (=, FW), (=, FW), (=, FW),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[(This, DT), (is, VBZ), (just, RB), (a, DT), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[(Whenever, WRB), (I, PRP), (offer, VBP), (to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[(If, IN), (I, PRP), (say, VBP), (it, PRP), (w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[(I, PRP), (ve, VBP), (always, RB), (said, VBD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Had, VBD), (the, DT), (signature, NN), (Blac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[(After, IN), (hitting, VBG), (up, RP), (the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Great, NNP), (happy, JJ), (hour, NN), (deals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>[(Fine, NNP), (Just, RB), (fine, JJ), (C+/B-, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>[(beautiful, JJ), (atmosphere, NN), (good, JJ)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Went, NNP), (there, RB), (tonight, NN), (For...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>[(This, DT), (is, VBZ), (the, DT), (quintessen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>[(Came, NN), (to, TO), (this, DT), (place, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>[(Black, NNP), (Chile, NNP), (Mexican, NNP), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>[(Pft, NNP), (Eh, NNP), (This, DT), (place, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>[(It, PRP), (s, VBZ), (definitely, RB), (not, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>[(The, DT), (food, NN), (here, RB), (is, VBZ),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>[(really, RB), (good, JJ), (gourmet, NNS), (Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>[(New, NNP), (restaurant, NN), (to, TO), (the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Fan-freaking-tastic, JJ), (shrimp, NN), (tac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Service, NNP), (is, VBZ), (very, RB), (frien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>[(My, PRP$), (husband, NN), (and, CC), (I, PRP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>[(My, PRP$), (girlfriend, NN), (and, CC), (I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Reading, VBG), (some, DT), (of, IN), (these,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>[(Good, JJ), (atmosphere, NN), (decent, JJ), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Really, RB), (great, JJ), (place, NN), (for,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>[(Our, PRP$), (server, NN), (Tom, NNP), (was, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>[(Was, NNP), (so, RB), (excited, VBD), (to, TO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>[(We, PRP), (had, VBD), (the, DT), (fish, NN),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19969</th>\n",
       "      <td>1</td>\n",
       "      <td>[(I, PRP), (seriously, RB), (doubted, VBD), (I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19970</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Just, RB), (had, VBD), (one, CD), (of, IN), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19971</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Freshest, NNP), (fish, NN), (and, CC), (a, D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19972</th>\n",
       "      <td>0</td>\n",
       "      <td>[(I, PRP), (personally, RB), (have, VBP), (no,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19973</th>\n",
       "      <td>1</td>\n",
       "      <td>[(After, IN), (about, IN), (5, CD), (visits, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19974</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Food, NN), (was, VBD), (great, JJ), (Service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19975</th>\n",
       "      <td>1</td>\n",
       "      <td>[(By, IN), (far, RB), (my, PRP$), (favorite, J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19976</th>\n",
       "      <td>1</td>\n",
       "      <td>[(The, DT), (Good, NNP), (the, DT), (sushi, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19977</th>\n",
       "      <td>0</td>\n",
       "      <td>[(Yasu, NNP), (is, VBZ), (usually, RB), (one, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19978</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Went, NNP), (here, RB), (with, IN), (my, PRP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19979</th>\n",
       "      <td>0</td>\n",
       "      <td>[(I, PRP), (would, MD), (give, VB), (a, DT), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19980</th>\n",
       "      <td>1</td>\n",
       "      <td>[(I, PRP), (absolutely, RB), (love, VBP), (Yas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19981</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Best, JJS), (sushi/sashimi, NN), (I, PRP), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19982</th>\n",
       "      <td>1</td>\n",
       "      <td>[(To, TO), (date, VB), (the, DT), (best, JJS),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19983</th>\n",
       "      <td>0</td>\n",
       "      <td>[(****Disclaimer****There, EX), (is, VBZ), (no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19984</th>\n",
       "      <td>1</td>\n",
       "      <td>[(This, DT), (is, VBZ), (the, DT), (best, JJS)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>1</td>\n",
       "      <td>[(I, PRP), (live, VBP), (in, IN), (what, WP), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19986</th>\n",
       "      <td>1</td>\n",
       "      <td>[(I, PRP), (d, MD), (have, VB), (to, TO), (say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>1</td>\n",
       "      <td>[(I, PRP), (read, VBP), (reviews, NNS), (on, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19988</th>\n",
       "      <td>1</td>\n",
       "      <td>[(OK, UH), (my, PRP$), (foodie, JJ), (friends,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19989</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Best, JJS), (Japanese, JJ), (restaurant, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19990</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Well, RB), (hate, NN), (to, TO), (join, VB),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991</th>\n",
       "      <td>0</td>\n",
       "      <td>[(Tucked, VBN), (into, IN), (a, DT), (small, J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19992</th>\n",
       "      <td>1</td>\n",
       "      <td>[(I, PRP), (have, VBP), (been, VBN), (looking,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19993</th>\n",
       "      <td>1</td>\n",
       "      <td>[(A, DT), (very, RB), (small, JJ), (local, JJ)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19994</th>\n",
       "      <td>1</td>\n",
       "      <td>[(my, PRP$), (first, JJ), (yelp, NN), (review,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0</td>\n",
       "      <td>[(Good, JJ), (Intimate, NNP), (Expensive, NNP)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>1</td>\n",
       "      <td>[(Great, JJ), (place, NN), (The, DT), (food, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0</td>\n",
       "      <td>[(The, DT), (location, NN), (is, VBZ), (conven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>1</td>\n",
       "      <td>[(I, PRP), (am, VBP), (a, DT), (sushi, JJ), (s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                                                  1\n",
       "0      0  [(This, DT), (location, NN), (is, VBZ), (out, ...\n",
       "1      0  [(=, SYM), (=, FW), (=, FW), (=, FW), (=, FW),...\n",
       "2      1  [(This, DT), (is, VBZ), (just, RB), (a, DT), (...\n",
       "3      0  [(Whenever, WRB), (I, PRP), (offer, VBP), (to,...\n",
       "4      0  [(If, IN), (I, PRP), (say, VBP), (it, PRP), (w...\n",
       "5      0  [(I, PRP), (ve, VBP), (always, RB), (said, VBD...\n",
       "6      1  [(Had, VBD), (the, DT), (signature, NN), (Blac...\n",
       "7      0  [(After, IN), (hitting, VBG), (up, RP), (the, ...\n",
       "8      1  [(Great, NNP), (happy, JJ), (hour, NN), (deals...\n",
       "9      0  [(Fine, NNP), (Just, RB), (fine, JJ), (C+/B-, ...\n",
       "10     1  [(beautiful, JJ), (atmosphere, NN), (good, JJ)...\n",
       "11     1  [(Went, NNP), (there, RB), (tonight, NN), (For...\n",
       "12     0  [(This, DT), (is, VBZ), (the, DT), (quintessen...\n",
       "13     0  [(Came, NN), (to, TO), (this, DT), (place, NN)...\n",
       "14     0  [(Black, NNP), (Chile, NNP), (Mexican, NNP), (...\n",
       "15     0  [(Pft, NNP), (Eh, NNP), (This, DT), (place, NN...\n",
       "16     0  [(It, PRP), (s, VBZ), (definitely, RB), (not, ...\n",
       "17     1  [(The, DT), (food, NN), (here, RB), (is, VBZ),...\n",
       "18     1  [(really, RB), (good, JJ), (gourmet, NNS), (Me...\n",
       "19     1  [(New, NNP), (restaurant, NN), (to, TO), (the,...\n",
       "20     1  [(Fan-freaking-tastic, JJ), (shrimp, NN), (tac...\n",
       "21     1  [(Service, NNP), (is, VBZ), (very, RB), (frien...\n",
       "22     0  [(My, PRP$), (husband, NN), (and, CC), (I, PRP...\n",
       "23     1  [(My, PRP$), (girlfriend, NN), (and, CC), (I, ...\n",
       "24     1  [(Reading, VBG), (some, DT), (of, IN), (these,...\n",
       "25     0  [(Good, JJ), (atmosphere, NN), (decent, JJ), (...\n",
       "26     1  [(Really, RB), (great, JJ), (place, NN), (for,...\n",
       "27     0  [(Our, PRP$), (server, NN), (Tom, NNP), (was, ...\n",
       "28     0  [(Was, NNP), (so, RB), (excited, VBD), (to, TO...\n",
       "29     1  [(We, PRP), (had, VBD), (the, DT), (fish, NN),...\n",
       "...   ..                                                ...\n",
       "19969  1  [(I, PRP), (seriously, RB), (doubted, VBD), (I...\n",
       "19970  1  [(Just, RB), (had, VBD), (one, CD), (of, IN), ...\n",
       "19971  1  [(Freshest, NNP), (fish, NN), (and, CC), (a, D...\n",
       "19972  0  [(I, PRP), (personally, RB), (have, VBP), (no,...\n",
       "19973  1  [(After, IN), (about, IN), (5, CD), (visits, N...\n",
       "19974  1  [(Food, NN), (was, VBD), (great, JJ), (Service...\n",
       "19975  1  [(By, IN), (far, RB), (my, PRP$), (favorite, J...\n",
       "19976  1  [(The, DT), (Good, NNP), (the, DT), (sushi, NN...\n",
       "19977  0  [(Yasu, NNP), (is, VBZ), (usually, RB), (one, ...\n",
       "19978  1  [(Went, NNP), (here, RB), (with, IN), (my, PRP...\n",
       "19979  0  [(I, PRP), (would, MD), (give, VB), (a, DT), (...\n",
       "19980  1  [(I, PRP), (absolutely, RB), (love, VBP), (Yas...\n",
       "19981  1  [(Best, JJS), (sushi/sashimi, NN), (I, PRP), (...\n",
       "19982  1  [(To, TO), (date, VB), (the, DT), (best, JJS),...\n",
       "19983  0  [(****Disclaimer****There, EX), (is, VBZ), (no...\n",
       "19984  1  [(This, DT), (is, VBZ), (the, DT), (best, JJS)...\n",
       "19985  1  [(I, PRP), (live, VBP), (in, IN), (what, WP), ...\n",
       "19986  1  [(I, PRP), (d, MD), (have, VB), (to, TO), (say...\n",
       "19987  1  [(I, PRP), (read, VBP), (reviews, NNS), (on, I...\n",
       "19988  1  [(OK, UH), (my, PRP$), (foodie, JJ), (friends,...\n",
       "19989  1  [(Best, JJS), (Japanese, JJ), (restaurant, NN)...\n",
       "19990  1  [(Well, RB), (hate, NN), (to, TO), (join, VB),...\n",
       "19991  0  [(Tucked, VBN), (into, IN), (a, DT), (small, J...\n",
       "19992  1  [(I, PRP), (have, VBP), (been, VBN), (looking,...\n",
       "19993  1  [(A, DT), (very, RB), (small, JJ), (local, JJ)...\n",
       "19994  1  [(my, PRP$), (first, JJ), (yelp, NN), (review,...\n",
       "19995  0    [(Good, JJ), (Intimate, NNP), (Expensive, NNP)]\n",
       "19996  1  [(Great, JJ), (place, NN), (The, DT), (food, N...\n",
       "19997  0  [(The, DT), (location, NN), (is, VBZ), (conven...\n",
       "19998  1  [(I, PRP), (am, VBP), (a, DT), (sushi, JJ), (s...\n",
       "\n",
       "[19999 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< Updated upstream
    "high_noun_2[:10]"
=======
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_high=list(df_high[1])\n",
    "list_low=list(df_low[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flat_high=list(itertools.chain.from_iterable(list_high))\n",
    "flat_low=list(itertools.chain.from_iterable(list_low))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flat_high_clean=pd.Series([a for a,b in flat_high if b==\"NN\" and a.isalpha()])\n",
    "flat_low_clean=pd.Series([a for a,b in flat_low if b==\"NN\" and a.isalpha()])"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fhc_vc=flat_high_clean.value_counts()\n",
    "flc_vc=flat_low_clean.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comp=pd.concat([fhc_vc,flc_vc], axis=1, join=\"outer\").fillna(0).rename(columns={0:\"High\",1:\"Low\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
