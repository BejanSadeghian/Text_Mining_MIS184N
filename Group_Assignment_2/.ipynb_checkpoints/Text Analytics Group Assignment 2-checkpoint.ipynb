{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 69,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
<<<<<<< HEAD
=======
    "import numpy as np\n",
>>>>>>> origin/Group-Assignment-1
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "from patsy import dmatrices\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import neighbors\n",
<<<<<<< HEAD
    "import numpy as np\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
=======
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit"
>>>>>>> origin/Group-Assignment-1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 14,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yelp = pd.read_csv(\"yelp.csv\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 15,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_mask = yelp['stars'] > 3\n",
    "yelp['High'] = 0\n",
    "yelp.ix[high_mask, 'High'] = 1"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 16,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "formula = 'High ~ 0 + votes_cool + votes_funny + votes_useful + Cheap + Moderate + Expensive  ' + \\\n",
    "' + VeryExpensive + American + Chinese + French + Japanese + Indian + Italian + Greek ' + \\\n",
    "' + Mediterranean + Mexican + Thai + Vietnamese + Others'"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 17,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y, X = dmatrices(formula, yelp, return_type='dataframe')\n",
    "y = Y['High'].values"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
=======
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = StratifiedShuffleSplit(y, n_iter = 1, test_size = 0.3, train_size = 0.7)"
>>>>>>> origin/Group-Assignment-1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_result = logistic_model.fit(X_train, y_train)"
=======
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StratifiedShuffleSplit(labels=[ 0.  0.  1. ...,  1.  0.  1.], n_iter=1, test_size=0.3, random_state=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DF_X = DataFrame(X)"
>>>>>>> origin/Group-Assignment-1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 60,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "0.684977498393\n"
=======
      "[18639  9783  2568 ...,  9408 10554  7261]\n"
>>>>>>> origin/Group-Assignment-1
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "logistic_train_prediction = logistic_model.predict(X_train)\n",
    "print metrics.accuracy_score(y_train, logistic_train_prediction)"
=======
    "for train_index, test_index in index:\n",
    "    print(train_index)\n",
    "    X_train, X_test = DF_X.iloc[train_index,], DF_X.iloc[test_index,]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
>>>>>>> origin/Group-Assignment-1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 61,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.685\n"
     ]
    }
   ],
   "source": [
    "logistic_test_prediction = logistic_model.predict(X_test)\n",
    "print metrics.accuracy_score(y_test, logistic_test_prediction)"
=======
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>votes_cool</th>\n",
       "      <th>votes_funny</th>\n",
       "      <th>votes_useful</th>\n",
       "      <th>Cheap</th>\n",
       "      <th>Moderate</th>\n",
       "      <th>Expensive</th>\n",
       "      <th>VeryExpensive</th>\n",
       "      <th>American</th>\n",
       "      <th>Chinese</th>\n",
       "      <th>French</th>\n",
       "      <th>Japanese</th>\n",
       "      <th>Indian</th>\n",
       "      <th>Italian</th>\n",
       "      <th>Greek</th>\n",
       "      <th>Mediterranean</th>\n",
       "      <th>Mexican</th>\n",
       "      <th>Thai</th>\n",
       "      <th>Vietnamese</th>\n",
       "      <th>Others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18639</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9783</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12730</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18368</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       votes_cool  votes_funny  votes_useful  Cheap  Moderate  Expensive  \\\n",
       "18639           0            0             1      0         1          0   \n",
       "9783            0            0             0      1         0          0   \n",
       "2568            1            0             1      0         1          0   \n",
       "12730           0            0             1      0         0          1   \n",
       "18368           0            0             0      0         1          0   \n",
       "\n",
       "       VeryExpensive  American  Chinese  French  Japanese  Indian  Italian  \\\n",
       "18639              0         0        0       0         0       0        1   \n",
       "9783               0         1        1       0         0       0        0   \n",
       "2568               0         0        0       0         0       0        0   \n",
       "12730              0         1        0       0         0       0        0   \n",
       "18368              0         0        0       0         0       0        0   \n",
       "\n",
       "       Greek  Mediterranean  Mexican  Thai  Vietnamese  Others  \n",
       "18639      0              0        0     0           0       0  \n",
       "9783       0              0        0     0           0       0  \n",
       "2568       0              0        1     0           0       0  \n",
       "12730      0              0        0     0           0       0  \n",
       "18368      0              0        1     0           0       0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
>>>>>>> origin/Group-Assignment-1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# KNN\n",
    "\n",
    "knn_model = neighbors.KNeighborsClassifier(n_neighbors=15, weights='uniform', p=2)\n",
    "knn_result = knn_model.fit(X_train, y_train)"
=======
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_result = logistic_model.fit(X_train, y_train)"
>>>>>>> origin/Group-Assignment-1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 17,
=======
   "execution_count": 63,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "0.687406243303\n"
=======
      "0.684048860633\n"
>>>>>>> origin/Group-Assignment-1
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "knn_train_prediction = knn_model.predict(X_train)\n",
    "print metrics.accuracy_score(y_train, knn_train_prediction)"
=======
    "logistic_train_prediction = logistic_model.predict(X_train)\n",
    "print metrics.accuracy_score(y_train, logistic_train_prediction)"
>>>>>>> origin/Group-Assignment-1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 18,
=======
   "execution_count": 64,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "0.6665\n"
=======
      "0.682\n"
>>>>>>> origin/Group-Assignment-1
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "knn_test_prediction = knn_model.predict(X_test)\n",
    "print metrics.accuracy_score(y_test, knn_test_prediction)"
=======
    "logistic_test_prediction = logistic_model.predict(X_test)\n",
    "print metrics.accuracy_score(y_test, logistic_test_prediction)"
>>>>>>> origin/Group-Assignment-1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": 65,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# Classification on text\n",
    "\n",
    "np.random.seed(1234567)\n",
    "train = yelp.sample(int(len(yelp)*0.7), replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = yelp[~yelp.index.isin(train.index.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = train['Review']\n",
    "train_y = train['High']\n",
    "test_x = test['Review']\n",
    "test_y = test['High']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, smooth_idf=True, strip_accents='unicode', norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_classification (v):\n",
    "    X_transform=v.fit_transform(train_x)\n",
    "    X_test=v.transform(test_x)\n",
    "    \n",
    "    nb_classifier = MultinomialNB().fit(X_transform, train_y)\n",
    "    y_nb_predicted = nb_classifier.predict(X_test)\n",
    "    \n",
    "    predict_y=Series(y_nb_predicted).reset_index()[0]\n",
    "    df=pd.DataFrame()\n",
    "    df['Predicted']=predict_y\n",
    "    df['Actual']=test_y.reset_index()['High']\n",
    "    \n",
    "    print \"Percent Correct\\n\",round((df['Predicted']==df['Actual']).mean()*100,3)\n",
    "    print \"\\nConfusion Matrix\\n\",pd.crosstab(index=df['Actual'],columns=df['Predicted'])\n",
    "    print \"\\nProportion Table\\n\", pd.crosstab(index=df['Actual'],columns=df['Predicted']).apply(lambda r: r/r.sum(), axis=1)"
=======
    "# KNN\n",
    "\n",
    "knn_model = neighbors.KNeighborsClassifier(n_neighbors=15, weights='uniform', p=2)\n",
    "knn_result = knn_model.fit(X_train, y_train)"
>>>>>>> origin/Group-Assignment-1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 24,
=======
   "execution_count": 66,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Percent Correct\n",
      "72.083\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted    0     1\n",
      "Actual              \n",
      "0          309  1668\n",
      "1            7  4016\n",
      "\n",
      "Proportion Table\n",
      "Predicted         0         1\n",
      "Actual                       \n",
      "0          0.156297  0.843703\n",
      "1          0.001740  0.998260\n"
=======
      "0.69312093721\n"
>>>>>>> origin/Group-Assignment-1
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "text_classification(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9530\n",
       "0    4469\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.value_counts()"
=======
    "knn_train_prediction = knn_model.predict(X_train)\n",
    "print metrics.accuracy_score(y_train, knn_train_prediction)"
>>>>>>> origin/Group-Assignment-1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 26,
=======
   "execution_count": 67,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
<<<<<<< HEAD
     "data": {
      "text/plain": [
       "1    4023\n",
       "0    1977\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.value_counts()"
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.674333333333\n"
     ]
    }
   ],
   "source": [
    "knn_test_prediction = knn_model.predict(X_test)\n",
    "print metrics.accuracy_score(y_test, knn_test_prediction)"
>>>>>>> origin/Group-Assignment-1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 69,
=======
   "execution_count": 70,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "# Text Classification with 50/50 splits\n",
    "\n",
    "highs = yelp[yelp['High']==1]\n",
    "lows = yelp[yelp['High']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_highs = highs.sample(6446, replace=False).copy()\n",
    "sample_lows = lows.sample(6446, replace=False).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = sample_highs.append(sample_lows, ignore_index=False)"
=======
    "# Classification on text\n",
    "\n",
    "np.random.seed(1234567)\n",
    "train = yelp.sample(int(len(yelp)*0.7), replace=False)"
>>>>>>> origin/Group-Assignment-1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 79,
=======
   "execution_count": 113,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "train = sample.sample(int(0.7*len(sample)), replace=False).copy()\n",
    "test = sample[~sample.index.isin(train.index.values)]"
=======
    "test = yelp[~yelp.index.isin(train.index.values)]"
>>>>>>> origin/Group-Assignment-1
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 80,
   "metadata": {
    "collapsed": false
=======
   "execution_count": 71,
   "metadata": {
    "collapsed": true
>>>>>>> origin/Group-Assignment-1
   },
   "outputs": [],
   "source": [
    "train_x = train['Review']\n",
    "train_y = train['High']\n",
    "test_x = test['Review']\n",
    "test_y = test['High']"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 81,
=======
   "execution_count": 72,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, smooth_idf=True, strip_accents='unicode', norm='l2')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 82,
=======
   "execution_count": 73,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_classification (v):\n",
    "    X_transform=v.fit_transform(train_x)\n",
    "    X_test=v.transform(test_x)\n",
    "    \n",
    "    nb_classifier = MultinomialNB().fit(X_transform, train_y)\n",
    "    y_nb_predicted = nb_classifier.predict(X_test)\n",
    "    \n",
    "    predict_y=Series(y_nb_predicted).reset_index()[0]\n",
    "    df=pd.DataFrame()\n",
    "    df['Predicted']=predict_y\n",
    "    df['Actual']=test_y.reset_index()['High']\n",
    "    \n",
    "    print \"Percent Correct\\n\",round((df['Predicted']==df['Actual']).mean()*100,3)\n",
    "    print \"\\nConfusion Matrix\\n\",pd.crosstab(index=df['Actual'],columns=df['Predicted'])\n",
    "    print \"\\nProportion Table\\n\", pd.crosstab(index=df['Actual'],columns=df['Predicted']).apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 83,
=======
   "execution_count": 74,
>>>>>>> origin/Group-Assignment-1
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent Correct\n",
      "81.903\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted     0     1\n",
      "Actual               \n",
      "0          1608   323\n",
      "1           377  1560\n",
      "\n",
      "Proportion Table\n",
      "Predicted         0         1\n",
      "Actual                       \n",
      "0          0.832729  0.167271\n",
      "1          0.194631  0.805369\n"
=======
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-50c3ba3de126>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_classification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-73-7b67ddbfe8ad>\u001b[0m in \u001b[0;36mtext_classification\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtext_classification\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mX_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnb_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_transform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\beins_000\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1283\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \"\"\"\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\beins_000\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    823\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m                                                        \u001b[0mmin_doc_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m                                                        max_features)\n\u001b[0m\u001b[0;32m    826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\beins_000\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.pyc\u001b[0m in \u001b[0;36m_limit_features\u001b[1;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[0mkept_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkept_indices\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m             raise ValueError(\"After pruning, no terms remain. Try a lower\"\n\u001b[0m\u001b[0;32m    721\u001b[0m                              \" min_df or a higher max_df.\")\n\u001b[0;32m    722\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkept_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremoved_terms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
>>>>>>> origin/Group-Assignment-1
     ]
    }
   ],
   "source": [
    "text_classification(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
<<<<<<< HEAD
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
=======
>>>>>>> origin/Group-Assignment-1
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
